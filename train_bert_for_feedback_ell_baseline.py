# -*- coding: utf-8 -*-
"""train-bert-for-feedback-ell-baseline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sbXTQ4iFYzq5H8KzuXShMaED_1bv4W9I
"""

import argparse
import pandas as pd
import gc
from torch.utils.data import DataLoader
from transformers import TrainingArguments, Trainer
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel
from torch.optim import AdamW
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding
from cfg import CFG
import torch.nn as nn
from dataset import TrainDataset
from model import Custom_Bert_Simple, Custom_Bert_Mean
from helper import *
from transformers.trainer_pt_utils import *
from transformers.trainer_utils import *
from transformers.integrations import *


# if is_sagemaker_mp_enabled():
#     import smdistributed.modelparallel.torch as smp

os.environ["WANDB_DISABLED"] = "true"



# def data_collator(batch):
#     input_ids = [{'input_ids': i[0]} for i in batch]
#     token_type_ids = [i[1] for i in batch]
#     attention_mask = [i[2] for i in batch]
#     labels = [i[3] for i in batch]
#     masked_input = mask_lm_datacollator(input_ids)['input_ids']
#     return masked_input, \
#            torch.stack(token_type_ids), \
#            torch.stack(attention_mask), \
#            torch.stack(labels)




# class CustomTrainer(Trainer):
#     def compute_loss(self, model, inputs, return_outputs=False):
#         # forward pass
#         loss, outputs = model(**inputs)
#         # compute custom loss (suppose one has 3 labels with different weights)
#         return (loss, outputs) if return_outputs else loss
#     def prediction_step(
#         self,
#         model: nn.Module,
#         inputs: Dict[str, Union[torch.Tensor, Any]],
#         prediction_loss_only: bool,
#         ignore_keys: Optional[List[str]] = None,
#     ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:
#         """
#         Perform an evaluation step on `model` using `inputs`.

#         Subclass and override to inject custom behavior.

#         Args:
#             model (`nn.Module`):
#                 The model to evaluate.
#             inputs (`Dict[str, Union[torch.Tensor, Any]]`):
#                 The inputs and targets of the model.

#                 The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
#                 argument `labels`. Check your model's documentation for all accepted arguments.
#             prediction_loss_only (`bool`):
#                 Whether or not to return the loss only.
#             ignore_keys (`Lst[str]`, *optional*):
#                 A list of keys in the output of your model (if it is a dictionary) that should be ignored when
#                 gathering predictions.

#         Return:
#             Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,
#             logits and labels (each being optional).
#         """
#         has_labels = all(inputs.get(k) is not None for k in self.label_names)
#         inputs = self._prepare_inputs(inputs)
#         if ignore_keys is None:
#             if hasattr(self.model, "config"):
#                 ignore_keys = getattr(self.model.config, "keys_to_ignore_at_inference", [])
#             else:
#                 ignore_keys = []

#         # labels may be popped when computing the loss (label smoothing for instance) so we grab them first.
#         if has_labels:
#             labels = nested_detach(tuple(inputs.get(name) for name in self.label_names))
#             if len(labels) == 1:
#                 labels = labels[0]
#         else:
#             labels = None

#         with torch.no_grad():
#             if is_sagemaker_mp_enabled():
#                 raw_outputs = smp_forward_only(model, inputs)
#                 if has_labels:
#                     if isinstance(raw_outputs, dict):
#                         loss_mb = raw_outputs["loss"]
#                         logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys + ["loss"])
#                     else:
#                         loss_mb = raw_outputs[0]
#                         logits_mb = raw_outputs[1:]

#                     loss = loss_mb.reduce_mean().detach().cpu()
#                     logits = smp_nested_concat(logits_mb)
#                 else:
#                     loss = None
#                     if isinstance(raw_outputs, dict):
#                         logits_mb = tuple(v for k, v in raw_outputs.items() if k not in ignore_keys)
#                     else:
#                         logits_mb = raw_outputs
#                     logits = smp_nested_concat(logits_mb)
#             else:
#                 if has_labels:
#                     with self.autocast_smart_context_manager():
#                         loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
#                     loss = loss.mean().detach()

#                     if isinstance(outputs, dict):
#                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys + ["loss"])
#                     else:
#                         logits = outputs#[1:]
#                 else:
#                     loss = None
#                     with self.autocast_smart_context_manager():
#                         outputs = model(**inputs)
#                     if isinstance(outputs, dict):
#                         logits = tuple(v for k, v in outputs.items() if k not in ignore_keys)
#                     else:
#                         logits = outputs
#                     # TODO: this needs to be fixed and made cleaner later.
#                     if self.args.past_index >= 0:
#                         self._past = outputs[self.args.past_index - 1]

#         if prediction_loss_only:
#             return (loss, None, None)

#         logits = nested_detach(logits)
#         if len(logits) == 1:
#             logits = logits[0]

#         return (loss, logits, labels)

def train_fn(train_loader, model, optimizer, epoch, scheduler, device):
    model.train()
    losses = AverageMeter()
    start = end = time.time()
    global_step = 0
    for step, batch in enumerate(train_loader):
        for key, value in batch.items():
            batch[key] = value.to(device)
        batch_size = CFG.batch_size
        loss, y_preds = model(**batch)
        losses.update(loss.item(), batch_size)
        optimizer.zero_grad()
        loss.backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)
        optimizer.step()
        global_step += 1
        scheduler.step()
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):
            print('Epoch: [{0}][{1}/{2}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  'Grad: {grad_norm:.4f}  '
                  'LR: {lr:.8f}  '
                  .format(epoch + 1, step, len(train_loader),
                          remain=timeSince(start, float(step + 1) / len(train_loader)),
                          loss=losses,
                          grad_norm=grad_norm,
                          lr=scheduler.get_lr()[0]))
    return losses.avg


def valid_fn(valid_loader, model, device):
    losses = AverageMeter()
    model.eval()
    preds = []
    labels = []
    start = end = time.time()
    for step, batch in enumerate(valid_loader):
        for key, value in batch.items():
            batch[key] = value.to(device)
        batch_size = CFG.batch_size
        with torch.no_grad():
            loss, y_preds = model(**batch)
        label = batch.get('labels')
        losses.update(loss.item(), batch_size)
        preds.append(y_preds.to('cpu').numpy())
        labels.append(label.to('cpu').numpy())
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):
            print('EVAL: [{0}/{1}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  .format(step, len(valid_loader),
                          loss=losses,
                          remain=timeSince(start, float(step + 1) / len(valid_loader))))
    predictions = np.concatenate(preds)
    labels = np.concatenate(labels)
    return losses.avg, predictions, labels



def train_loop(fold, train_dataset, valid_dataset):
    LOGGER = get_logger()
    LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))
    LOGGER.info('===============seed_{}==============='.format(CFG.seed))
    LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))
    LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))
    LOGGER.info(f"========== training ==========")

    # ====================================================
    # loader
    # ====================================================

    train_loader = DataLoader(train_dataset,
                              batch_size=CFG.batch_size,
                              shuffle=True,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)
    valid_loader = DataLoader(valid_dataset,
                              batch_size=CFG.batch_size * 2,
                              shuffle=False,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)

    # ====================================================
    # model & optimizer
    # ====================================================
    model = Custom_Bert_Mean()
    # model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)
    model.to(CFG.device)

    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):

        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': weight_decay},
            {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': 0.0},
            {'params': [p for n, p in model.named_parameters() if 'backbone' not in n],
             'lr': decoder_lr, 'weight_decay': weight_decay}
        ]
        return optimizer_parameters

    optimizer_parameters = get_optimizer_params(model,
                                                encoder_lr=CFG.encoder_lr,
                                                decoder_lr=CFG.decoder_lr,
                                                weight_decay=CFG.weight_decay)
    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)

    # ====================================================
    # scheduler
    # ====================================================
    def get_scheduler(cfg, optimizer, num_train_steps):
        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps
        if cfg.scheduler == 'linear':
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps
            )
        elif cfg.scheduler == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,
                num_cycles=cfg.num_cycles
            )
        return scheduler

    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)
    scheduler = get_scheduler(CFG, optimizer, num_train_steps)


    best_score = 0.

    for epoch in range(CFG.epochs):

        start_time = time.time()

        # train
        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)

        # eval
        avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)

        # scoring
        score = MCRMSE(valid_labels, predictions)

        elapsed = time.time() - start_time

        LOGGER.info(
            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')
        LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f}')

        if best_score < score:
            best_score = score
            best_predictions = predictions
            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')
            torch.save({'model': model.state_dict(),
                        'predictions': predictions},
                       CFG.OUTPUT_DIR + "{}_best{}.pth".format(CFG.model_path.replace('/', '_'), fold))

    torch.cuda.empty_cache()
    gc.collect()
    del scheduler, optimizer, model
    return best_predictions


def main():
    train_df = pd.read_csv('train_df_with_fold.csv')
    # train_df = train_df.sample(n=60)

    tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)
    seed_everything(CFG.seed)
    for fold in [0]:
        #fold = CFG.training_fold
        tr_data = train_df[train_df['kfold'] != fold].reset_index(drop=True)
        va_data = train_df[train_df['kfold'] == fold].reset_index(drop=True)
        tr_dataset = TrainDataset(tr_data, tokenizer)
        va_dataset = TrainDataset(va_data, tokenizer)
        val_result = train_loop(fold, tr_dataset, va_dataset)
    # model = Custom_Bert_Mean_with_GRU()
    # args = TrainingArguments(
    #     CFG.OUTPUT_DIR,
    #     evaluation_strategy="steps",
    #     learning_rate=2e-5,
    #     eval_steps=50,
    #     save_steps=50,
    #     per_device_train_batch_size=CFG.batch_size,
    #     per_device_eval_batch_size=CFG.batch_size*2,
    #     num_train_epochs=4,
    #     weight_decay=0.01,
    #     greater_is_better=False,
    #     save_total_limit=10,
    #     load_best_model_at_end=True,
    #     metric_for_best_model='mcrmse',
    #     warmup_ratio=0.1,
    #     lr_scheduler_type='cosine'
    # )
    #
    # trainer = CustomTrainer(
    #     model,
    #     args,
    #     train_dataset=tr_dataset,
    #     eval_dataset=va_dataset,
    #     tokenizer=tokenizer,
    #     compute_metrics=compute_metrics
    # )
    # trainer.train()
    # torch.save(model.state_dict(),CFG.OUTPUT_DIR + "{}_best{}.pth".format(CFG.model_path.replace('/', '_'), fold))
    print('done')

if __name__ == '__main__':
    main()

